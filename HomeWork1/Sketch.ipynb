{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    " \n",
    "This homework will explore some basic techniques presented in the first two weeks of the course.\n",
    " \n",
    "You will be invited to experiment with:\n",
    " \n",
    " 1. Word Frequencies and Generation\n",
    " 2. Co-occurrences\n",
    " 2. Zipf's law\n",
    " 2. Naive Bayes classifier\n",
    " \n",
    " \n",
    "Fill in the blanks with your own code and see how it works.\n",
    " \n",
    "# 1. Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of words of documents in the category `news`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_news = brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the frequencies for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_freq = # this should be a dictionary with frequency counts for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a probability distribution of all words from `news_freq`, sorted by decreasing probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_prob = # probability distribution for all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which returns a random word according to the above probability distribution. <p>\n",
    "Hint: use function `random.random` to obtain a random number in the range `[0, 1]` and scan the probabilities from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(prob_dist):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the vocabulary and assign an index to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocab = # this should be a set\n",
    "\n",
    "news_word_ids = # this should be a dictionary that assigns an index to each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute co-occurrence Matrix\n",
    "Constructs a co-occurrence matrix for a certain window-size $n$ (with a default of\n",
    "4), considering words $n$ before and $n$ after the word in the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def co_occurrence_matrix(words, word_ids, n=4):\n",
    "    \"\"\"\n",
    "    :param words: a list of words\n",
    "    :param word_ids: dictionary word -> id\n",
    "    :param n: size of context\n",
    "    :return: the matrix of cooccurrences\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = co_occurrence_matrix(brown_news, news_word_ids, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cooccurrences(word_list, word_ids, M):\n",
    "    \"\"\"\n",
    "    :param word_list: the list of words whose co-occurrences to show\n",
    "    :param word_ids: dictionary assigning ID's to words\n",
    "    :param M: the co-occurrence matrix\n",
    "    \"\"\"\n",
    "    # print the top 10 most co-occurring words for each word in word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['python', 'snake', 'language', 'cobra', 'food', 'bread', 'wine', 'beer',\n",
    "             'barrels', 'oil', 'energy', 'kuwait', 'revenues', 'profits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cooccurrences(word_list, news_word_ids, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for category ‘hobbies’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_hobbies = brown.words(categories='hobbies')\n",
    "hobbies_vocab = # ...\n",
    "\n",
    "hobbies_word_ids = # ...\n",
    "M_hobbies = # ...\n",
    "\n",
    "show_cooccurrences(word_list, hobbies_word_ids, M_hobbies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualize the Vectors\n",
    "Projects vectors into two dimensions and use it to plot the rows of the sliced matrix, using the technique of Principal\n",
    "Component Analysis,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def visualize_vectors(words, word_vectors):\n",
    "    \"\"\"\n",
    "    Plot a 2 dimensional visualization of word vectors.\n",
    "    :param words: the words to visualize\n",
    "    :param word_vectors: the vectors for those words\n",
    "    \"\"\"\n",
    "    vector_twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    # show the numerical values of reduced vectors\n",
    "    for w,v in zip(words, vector_twodim):\n",
    "        print(w, v)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(vector_twodim[:,0], vector_twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, vector_twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the co-occurrence vectors for a given list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_vectors(word_list, word_ids, M):\n",
    "    \"\"\"\n",
    "    :param word_list: a list of words\n",
    "    :param word_ids: dictionary of word IDs\n",
    "    :param M: co-occurrence matrix\n",
    "    :return: tow results: the vectors for the given words as well as word_list \n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    # ...\n",
    "    return vectors, word_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the co-occurrence vectors for the given words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, words = cooccurrence_vectors(word_list, news_word_ids, M)\n",
    "visualize_vectors(words, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, wordsw = cooccurrence_vectors(word_list, hobbies_word_ids, M_hobbies)\n",
    "visualize_vectors(words, word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define two functions, one to tokenize text from a .txt file and another to extract the vocabulary and compute the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_from_txt(file_name):\n",
    "    from nltk import word_tokenize\n",
    "    ...            \n",
    "    return words\n",
    "\n",
    "def count_words(words):\n",
    "    vocab = dict()\n",
    "    ...\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Extract the vocabulary and the number of occurrences of each word in it and plot the rank. Then fit the power law distribution to the data to extract the exponent alpha of the power law distribution and use it to estimate the exponent beta of the Zipf's law that approsimate the data.\n",
    "Zipf's law:\n",
    "\n",
    "$X[r] = C*r^{-\\beta}$\n",
    "\n",
    "where r is the rank and C a constant. The estimation of C should be equal to the number of occurrences in of the most frequent word but it's easy to see that a bigger value is needed to better fit the Zipf's law.\n",
    "\n",
    "Regardless of the constant C the plot shows that the rank distribution is well approximated by the Zip's law.\n",
    "\n",
    "You may want to use the package powerlaw to fit the \\alpha to the data, from which to compute:\n",
    "$$\\beta = 1/(\\alpha - 1)$$\n",
    "\n",
    "See: https://github.com/jeffalstott/powerlaw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_name = '../data/war_and_peace.txt'\n",
    "\n",
    "# extracting rank distribution of the words\n",
    "# ...\n",
    "# fitting of the power law distribution and estimation of the exponent of the Zip's law\n",
    "import powerlaw\n",
    "# ...\n",
    "# plot the Zipf's law and the rank distribution on a log-log scale\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naïve Bayes Classifier\n",
    "We will use the Movie Review dataset from https://www.kaggle.com/pankrzysiu/keras-imdb\n",
    "It contains 50,000 highly polarized reviews, preprocessed, tokenized, indexed and stored into\n",
    "numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_NUM_WORDS,\n",
    "                                                     index_from=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is represented as a list of word IDs, while the output is {0,1} (negative or positive review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Classifier\n",
    "\n",
    "The classifier should be implemted as a class with two methods: `train()` and `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class NBayesClassifier():\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        :param x_train: an array of list of word IDs\n",
    "        :param y_train: {0,1} category for the correspoding document\n",
    "        \"\"\"\n",
    "        # compute priors for each category\n",
    "        self.P_C = #...\n",
    "        # compute the conditional probabilities for each word/category pair\n",
    "        self.logprob = # ...\n",
    "        # turn counts into logprob\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        :param x_test: an array of list of word IDs\n",
    "        :return: the predicted class for each document in :param x_test:.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = NBayesClassifier()\n",
    "nb_classifier.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    # compute tp, fp, tn, fn\n",
    "    accuracy = # ...\n",
    "    precision = # ...\n",
    "    recall = # ...\n",
    "    F1 = # ...\n",
    "    print('Accuracy: %.2f %%, Precision:: %2.f %%, Recall: %2.f %%, F1: %2.f %%' % (accuracy, precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(x_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with an official metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
