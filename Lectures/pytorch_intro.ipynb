{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "Overview of the main features of PyTorch:\n",
    "- autograd\n",
    "- dynamic computation graph\n",
    "- model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "In Numpy, you may have an array that has three dimensions. That is, technically speaking, a tensor. <br/>\n",
    "A **scalar** (a single number) has zero dimensions, a **vector** has one dimension, a **matrix** has two dimensions and a **tensor** has three or more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones([2,2])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "What distinguishes a `tensor` used for data — like the ones we’ve just created — from a tensor used as a (trainable) `parameter`/`weight`?\n",
    "The latter tensors require the computation of its gradients, so we can update their values (the parameters’ values, that is). That’s what the requires_grad=True argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "`torch.Tensor` is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE. In PyTorch, every method that ends with an underscore (_) makes changes in-place, meaning, they will modify the underlying variable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or cerate directly as a paramenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones([2,2], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parameter mantains the history of how it was created, the expression that created, or the computation graph that produced it.\n",
    "\n",
    "This supports automatic differentiation by means of `Autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about partial derivatives, chain rule or anything like it.\n",
    "So, how do we tell PyTorch to do its thing and compute all gradients?<br/>\n",
    "That’s what the method `backward()` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "y: tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<AddBackward0>)\n",
      "z: tensor([[12., 12.],\n",
      "        [12., 12.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([2,2], requires_grad=True)\n",
    "y = x + 1\n",
    "z = y * y * 3\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('z:', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"100pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 100.00 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-96 96,-96 96,4 -4,4\"/>\n",
       "<!-- 139905884520520 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139905884520520</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"92,-21 0,-21 0,0 92,0 92,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139905884523544 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139905884523544</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"73,-92 19,-92 19,-57 73,-57 73,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 139905884523544&#45;&gt;139905884520520 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139905884523544&#45;&gt;139905884520520</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M46,-56.6724C46,-48.8405 46,-39.5893 46,-31.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"49.5001,-31.2234 46,-21.2234 42.5001,-31.2235 49.5001,-31.2234\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3e608c8a90>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones([2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of<br>\n",
    "$z = (x+1)(x+1)3$ <br>\n",
    "is <br>\n",
    "$\\delta{z}/\\delta{x} = 3x(x+1)+3(x+1)x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12.],\n",
       "        [12., 12.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[18., 18.],\n",
       "        [18., 18.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z.backward(torch.ones([2,2]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "y: tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "z: tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "o: tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "o = z.mean()\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('z:', z)\n",
    "print('o:', o)\n",
    "o.backward(torch.tensor(1.))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o = \\frac{1}{4}\\sum{i}{z_i}$ <br>\n",
    "$z_i = 3(x+2)^2$ <br>\n",
    "$\\frac{\\delta o}{\\delta x_i} = \\frac{1}{4} \\cdot 3 \\cdot 2 (x_i+2) = \\frac{3}{2}(x_i+2) = \\frac{9}{2} = 4.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x_train, y_train, s=10, c=('blue'), alpha=0.5)\n",
    "plt.title('Train data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pt = torch.from_numpy(x_train).float()\n",
    "y_train_pt = torch.from_numpy(y_train).float()\n",
    "\n",
    "x_val_pt = torch.from_numpy(x_val).float()\n",
    "y_val_pt = torch.from_numpy(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_pt\n",
    "error = y_train_pt - yhat\n",
    "loss = (error ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 139905884500208 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139905884500208</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139905884503512 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139905884503512</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139905884503512&#45;&gt;139905884500208 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139905884503512&#45;&gt;139905884500208</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 139905884502840 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139905884502840</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139905884502840&#45;&gt;139905884500208 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139905884502840&#45;&gt;139905884500208</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 139905884502000 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139905884502000</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139905884502000&#45;&gt;139905884502840 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139905884502000&#45;&gt;139905884502840</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3e608c3da0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"342pt\"\n",
       " viewBox=\"0.00 0.00 171.50 342.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-338 167.5,-338 167.5,4 -4,4\"/>\n",
       "<!-- 139905935107912 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139905935107912</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"121,-21 23,-21 23,0 121,0 121,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 139905935106232 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139905935106232</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118.5,-78 25.5,-78 25.5,-57 118.5,-57 118.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 139905935106232&#45;&gt;139905935107912 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139905935106232&#45;&gt;139905935107912</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 139905935107968 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139905935107968</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"117,-135 27,-135 27,-114 117,-114 117,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 139905935107968&#45;&gt;139905935106232 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139905935107968&#45;&gt;139905935106232</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-113.7787C72,-106.6134 72,-96.9517 72,-88.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-88.1732 72,-78.1732 68.5001,-88.1732 75.5001,-88.1732\"/>\n",
       "</g>\n",
       "<!-- 139905884692664 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139905884692664</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118,-192 26,-192 26,-171 118,-171 118,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139905884692664&#45;&gt;139905935107968 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139905884692664&#45;&gt;139905935107968</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-170.7787C72,-163.6134 72,-153.9517 72,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-145.1732 72,-135.1732 68.5001,-145.1732 75.5001,-145.1732\"/>\n",
       "</g>\n",
       "<!-- 139905884694512 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139905884694512</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-263 0,-263 0,-228 54,-228 54,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139905884694512&#45;&gt;139905884692664 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139905884694512&#45;&gt;139905884692664</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-227.6724C45.4798,-219.2176 52.5878,-209.1085 58.6352,-200.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-202.4169 64.4601,-192.2234 55.8452,-198.3906 61.5714,-202.4169\"/>\n",
       "</g>\n",
       "<!-- 139905884694736 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139905884694736</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-256 72.5,-256 72.5,-235 163.5,-235 163.5,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139905884694736&#45;&gt;139905884692664 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139905884694736&#45;&gt;139905884692664</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-234.9317C103.7191,-225.6309 93.821,-211.8597 85.7479,-200.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-198.3753 79.761,-192.2979 82.7553,-202.4608 88.4395,-198.3753\"/>\n",
       "</g>\n",
       "<!-- 139905884358248 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139905884358248</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-334 91,-334 91,-299 145,-299 145,-334\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139905884358248&#45;&gt;139905884694736 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139905884358248&#45;&gt;139905884694736</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-298.9494C118,-289.058 118,-276.6435 118,-266.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-266.0288 118,-256.0288 114.5001,-266.0289 121.5001,-266.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3e63906828>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at its components:\n",
    "- **blue** boxes: these correspond to the tensors we use as parameters, the ones we’re asking PyTorch to compute gradients for;\n",
    "- **gray** box: a Python operation that involves a gradient-computing tensor or its dependencies;\n",
    "- **green** box: the same as the gray box, except it is the starting point for the computation of gradients (assuming the backward()method is called from the variable used to visualize the graph)— they are computed from the bottom-up in a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "We can perfomr linera regression on the data by gradient descent, in order to minimize the mean suare loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_pt\n",
    "    error = y_train_pt - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We can perform interpolation using an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_pt\n",
    "    error = y_train_pt - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    # Gradient\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "In PyTorch, a model is represented by a regular Python class that inherits from the Module class.\n",
    "The most fundamental methods it needs to implement are:\n",
    "- __init__(self): it defines the parts that make up the model —in our case, two parameters, a and b.\n",
    "\n",
    "*NOTE: You are not limited to defining parameters, though… models can contain other models (or layers) as its attributes as well, so you can easily nest them. We’ll see an example of this shortly as well.*\n",
    "\n",
    "- forward(self, x): it performs the actual computation, that is, it outputs a prediction, given the input x.\n",
    "\n",
    "*NOTE. You should NOT call the forward(x) method, though. You should call the whole model itself, as in model(x) to perform a forward pass and output predictions.*\n",
    "\n",
    "Let’s build a proper (yet simple) model for our regression task. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = LinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # What is this?!?\n",
    "    model.train()\n",
    "\n",
    "    yhat = model(x_train_pt)\n",
    "    \n",
    "    loss = loss_fn(y_train_pt, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use PyTorch’s Linear model as an attribute of our own, thus creating a nested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "Simiarly to `Keras`, one can use a `Sequential` model, to define\n",
    "straightforward models, that use run-of-the-mill layers, where the output of a layer is sequentially fed as an input to the next.\n",
    "\n",
    "In our case, we would build a `Sequential` model with a single argument, that is, the `Linear` layer we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
