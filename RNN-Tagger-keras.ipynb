{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagger\n",
    "\n",
    "This example trains a RNN to tag words from a corpus.\n",
    "\n",
    "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "SENTENCE_LENGTH_MAX = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text and Parsing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Wikipedia Corpus\n",
    "\n",
    "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "Here's the paper that explains how the corpus was constructed : \n",
    "\n",
    "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_file = './data/en.wikipedia.2010.100K.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus available locally\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(corpus_text_file):\n",
    "    raise RuntimeError(\"You need to download the corpus file : \"+\n",
    "                       \"http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_wikipedia_2010_100K.tar.gz\")\n",
    "else:\n",
    "    print(\"Corpus available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_sentence_tokens(corpus_text_file):\n",
    "    while True:\n",
    "        with open(corpus_text_file, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
    "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
    "                    tokens = tokenizer.tokenize(s)\n",
    "                    if len(tokens) < SENTENCE_LENGTH_MAX:\n",
    "                        yield tokens\n",
    "        print(\"Corpus : Looping\")\n",
    "corpus_sentence_tokens_gen = corpus_sentence_tokens(corpus_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Showing | that | even | in | the | modern | warfare | of | the | 1930s | and | 1940s | , | the | dilapidated | fortifications | still | had | defensive | usefulness | .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' | '.join(next(corpus_sentence_tokens_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RP | ( | RB | VBG | PDT | VBP | RBR | IN | . | ) | VBD | NNS | EX | RBS | NNP | DT | POS | $ | , | `` | UH | : | WP | WRB | # | VB | VBN | NN | WDT | TO | CC | NNPS | WP$ | CD | PRP | '' | JJ | JJS | FW | PRP$ | VBZ | MD | SYM | LS | JJR\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pos_tagger = PerceptronTagger(load=True)\n",
    "' | '.join(list(pos_tagger.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('see', 'VB'),\n",
       " ('what', 'WP'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('analysis', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Jeff', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('sample', 'NN'),\n",
       " ('text', 'JJ'),\n",
       " ('looks', 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Let 's see what part of speech analysis on Jeff 's sample text looks like .\".split(' ')\n",
    "#s = next(corpus_sentence_tokens_gen)\n",
    "pos_tagger.tag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twist : Not interested in all classes...\n",
    "\n",
    "To simplify (dramatically), our RNN will be trained to just tell the difference between 'is ordinary word' and 'is entity name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = ['O', 'E']\n",
    "pos_tagger_entity_tags = set(['NNP'])\n",
    "pos_tagger_to_idx = dict([(t, int(t in pos_tagger_entity_tags)) \n",
    "                            for i,t in enumerate(pos_tagger.classes)])\n",
    "TAG_SET_SIZE= len(tag_list)\n",
    "\n",
    "pos_tagger_to_idx['NNP'], pos_tagger_to_idx['VBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVE available locally\n"
     ]
    }
   ],
   "source": [
    "glove_100k_50d_path = './data/glove.6B/glove.first-100k.6B.50d.txt'\n",
    "\n",
    "if not os.path.isfile(glove_100k_50d_path):\n",
    "    raise RuntimeError(\"You need to download GloVE Embeddings \"+\n",
    "                       \"from http://nlp.stanford.edu/data/\")\n",
    "else:\n",
    "    print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glove\n",
    "word_embedding = glove.Glove.load_stanford(glove_100k_50d_path)\n",
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An RNN Part-of-Speech Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the Embedding  Keras-Compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100002, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = word_embedding.word_vectors.shape[1]\n",
    "word_embeddings = np.vstack([ \n",
    "        np.zeros((1, EMBEDDING_DIM,), dtype='float32'),   # This is the 'zero' value (used as a mask in Keras)\n",
    "        np.zeros((1, EMBEDDING_DIM,), dtype='float32'),   # This is for 'UNK'  (word == 1)\n",
    "        word_embedding.word_vectors,\n",
    "    ])\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesising a 'correct answer' for the Tagger\n",
    "\n",
    "Normally, this would be the (manual) annotations from the corpus itself.  However, we don't have an annotated corpus.  Instead, we're going to use the annotations produced by the NTLK tagger - simplified to only identify 'NNP = entities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def word_to_idx_rnn(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)  # since UNK=1 = (-1+2)\n",
    "    return idx+2  # skip ahead 2 places\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def sentences_for_network(sentences, include_targets=False, one_hot_targets=False):\n",
    "    \"\"\"\n",
    "    sentences: list of sentences\n",
    "    include_targets:\n",
    "    one_hot_target:\n",
    "    \"\"\"\n",
    "    len_of_list = len(sentences)\n",
    "    #print(\"sentences_for_network.sentences.length = %d\" % (len_of_list,))\n",
    "    \n",
    "    input_values = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for j, word in enumerate(sent):\n",
    "            input_values[i,j] = word_to_idx_rnn(word)\n",
    "    \n",
    "    if not include_targets: \n",
    "        return (input_values, None)\n",
    "\n",
    "    if one_hot_targets:\n",
    "        # Add extra dimension here to suit Keras' TimeDistributed(Dense(softmax))\n",
    "        #   as discussed : https://github.com/fchollet/keras/issues/6363\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX, TAG_SET_SIZE), dtype='int32')\n",
    "    else:\n",
    "        target_values  = np.zeros((len_of_list, SENTENCE_LENGTH_MAX), dtype='int32')\n",
    "        \n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentence_tags = pos_tagger.tag(sent)\n",
    "        for j, word_tag in enumerate(sentence_tags):\n",
    "            tag = word_tag[1] # tags are returned as tuples (word, tag)\n",
    "            pos_class = pos_tagger_to_idx[tag]  # These are the class #s\n",
    "            if one_hot_targets:\n",
    "                target_values[i,j] = to_categorical(pos_class, num_classes=TAG_SET_SIZE)\n",
    "            else:\n",
    "                target_values[i,j] = pos_class\n",
    "\n",
    "    return (input_values, target_values)\n",
    "\n",
    "def batch_for_network_generator():\n",
    "    while True:\n",
    "        batch_of_sentences = [ next(corpus_sentence_tokens_gen) for i in range(BATCH_SIZE) ]    \n",
    "        yield sentences_for_network(batch_of_sentences, include_targets=True, one_hot_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the batchifier\n",
    "\n",
    "This just finds the next values to be produced - it isn't needed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 32), (64, 32, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch_input, single_batch_targets = next(batch_for_network_generator())\n",
    "single_batch_input.shape, single_batch_targets.shape\n",
    "#single_batch_input[0]\n",
    "#single_batch_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Symbolically\n",
    "\n",
    "#### Good blog post series\n",
    "*  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
    "\n",
    "#### Keras Examples\n",
    "*  https://github.com/fchollet/keras/issues/5022\n",
    "\n",
    "#### Keras RNN\n",
    "*  https://keras.io/layers/recurrent/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, GRU, Dense #, Activation\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_HIDDEN_SIZE = word_embeddings.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_embeddings.shape[0],\n",
    "                    word_embeddings.shape[1],\n",
    "                    weights=[word_embeddings],\n",
    "                    input_length=SENTENCE_LENGTH_MAX,\n",
    "                    trainable=False, \n",
    "                    mask_zero=True,\n",
    "                    name=\"SentencesEmbedded\"))\n",
    "\n",
    "# Gated Recurrent Unit\n",
    "model.add(Bidirectional(GRU(RNN_HIDDEN_SIZE, return_sequences=True),\n",
    "                        merge_mode='concat'))\n",
    "\n",
    "# Fully connected layer\n",
    "# RNN_HIDDEN_SIZE*2 because the bidirectional GRU are concatenated\n",
    "model.add(TimeDistributed(Dense(TAG_SET_SIZE, activation='softmax'), \n",
    "                          input_shape=(BATCH_SIZE, SENTENCE_LENGTH_MAX, RNN_HIDDEN_SIZE*2),\n",
    "                          name='POS-class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "SentencesEmbedded (Embedding (None, 32, 50)            5000100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 100)           30300     \n",
      "_________________________________________________________________\n",
      "POS-class (TimeDistributed)  (None, 32, 2)             202       \n",
      "=================================================================\n",
      "Total params: 5,030,602\n",
      "Trainable params: 30,502\n",
      "Non-trainable params: 5,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")  # , metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase for the RNN\n",
    "\n",
    "This will actually **train** the RNN - which can take 3-5 minutes (depending on your CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "500/500 [==============================] - 61s 122ms/step - loss: 0.17908 - ETA: 6 - ETA: 4s - loss: 0.1 - ETA: 4s - loss: 0 - ETA: 0s - loss: 0.179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa513f91c88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(x, y_one_hot)\n",
    "model.fit_generator(batch_for_network_generator(), 500, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tagger Network 'works'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_results_for(test_sentences):\n",
    "    #sentences_for_network(sentences, include_targets=False, one_hot_targets=False)\n",
    "    input_values, target_values_int = sentences_for_network(test_sentences, include_targets=True)\n",
    "\n",
    "    rnn_output = model.predict_on_batch(input_values)\n",
    "\n",
    "    # rnn_output here is a softmax-vector at every word location\n",
    "    for i,sent in enumerate(test_sentences): # [0:5]):\n",
    "        annotated = [ \n",
    "                \"%s-%d-%d\" % (word, target_values_int[i,j], np.argmax(rnn_output[i,j]), )    \n",
    "                for j,word in enumerate(sent) \n",
    "            ]\n",
    "        print(' '.join(annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format: WORD-NLTK-RNN\n",
      "\n",
      "Dr.-1-1 Andrews-1-1 works-0-0 at-0-0 Red-1-1 Cat-1-1 Labs-1-1 .-0-0\n",
      "Let-0-0 's-0-0 see-0-0 what-0-0 part-0-0 of-0-0 speech-0-0 analysis-0-0 looks-0-0 like-0-0 .-0-0\n",
      "When-0-0 are-0-0 you-0-0 off-0-0 to-0-0 New-1-0 York-1-1 ,-0-0 Chaitanya-1-1 ?-0-0\n",
      "\n",
      "Dr.-1-1 Andrews-1-1 Works-1-0 At-0-0 Red-1-1 Cat-1-1 Labs-1-1 .-0-0\n",
      "Let-0-0 'S-0-0 See-0-0 What-0-0 Part-1-0 Of-0-0 Speech-1-0 Analysis-1-0 Looks-1-0 Like-0-0 .-0-0\n",
      "When-0-0 Are-1-0 You-0-0 Off-1-0 To-0-0 New-1-0 York-1-1 ,-0-0 Chaitanya-1-1 ?-0-0\n",
      "\n",
      "dr.-0-1 andrews-0-1 works-0-0 at-0-0 red-0-1 cat-0-1 labs-0-1 .-0-0\n",
      "let-0-0 's-0-0 see-0-0 what-0-0 part-0-0 of-0-0 speech-0-0 analysis-0-0 looks-0-0 like-0-0 .-0-0\n",
      "when-0-0 are-0-0 you-0-0 off-0-0 to-0-0 new-0-0 york-0-1 ,-0-0 chaitanya-0-1 ?-0-0\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Dr. Andrews works at Red Cat Labs .\",\n",
    "    \"Let 's see what part of speech analysis looks like .\",\n",
    "    \"When are you off to New York , Chaitanya ?\",\n",
    "]\n",
    "\n",
    "# Uncomment this for 8 sentences from the corpus\n",
    "#sentences = [ ' '.join(next(corpus_sentence_tokens_gen)) for i in range(8) ]\n",
    "\n",
    "test_sentences_mixed = [ s.split(' ') for s in sentences ]\n",
    "test_sentences_title = [ s.title().split(' ') for s in sentences ]\n",
    "test_sentences_single = [ s.lower().split(' ') for s in sentences ]\n",
    "#test_sentences_single = [ s.upper().split(' ') for s in sentences ]\n",
    "\n",
    "print(\"Format: WORD-NLTK-RNN\\n\")\n",
    "\n",
    "tag_results_for(test_sentences_mixed)\n",
    "print()\n",
    "tag_results_for(test_sentences_title)\n",
    "print()\n",
    "tag_results_for(test_sentences_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  And let's look at the Statistics\n",
    "\n",
    "... actually, looking at the above samples, the NLTK PoS tagger is HOPELESS when the text is converted to a single case, or title case. QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1.  Make the tagger identify different PoS (say : 'verbs')\n",
    "\n",
    "2.  Make the tagger return several different tags instead\n",
    "\n",
    "3.  See whether more advanced 'LSTM' nodes would improve the scores\n",
    "\n",
    "4.  Add a special 'is_uppercase' element to the embedding vector (or, more simply, just replace one of the elements with an indicator).  Does this help the NNP accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
